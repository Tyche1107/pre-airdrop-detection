# Pre-Airdrop Sybil Detection

## Four Core Findings

**1. Pre-airdrop detection outperforms post-hoc GNN.**
Using only on-chain behavior strictly before the distribution event, LightGBM achieves AUC 0.905 at T-30 on Blur and AUC 0.946 at T-30 on LayerZero — both exceeding ARTEMIS (0.803), which uses full post-hoc graph data including transfer edges generated by the airdrop itself.

**2. Graph neural networks fail structurally before the airdrop.**
ArtemisNet (AUC 0.976 post-hoc) collapses to AUC 0.586 at T-30. The cause is architectural: GNN edges include transfer events that only exist after distribution. Remove those edges and the graph is structurally incomplete. This is not a tuning problem — it is a fundamental incompatibility between graph methods and the pre-airdrop setting.

**3. The behavioral signal is stable months in advance.**
Blur: AUC drops only 0.013 from T-0 to T-180. LayerZero: AUC range is 0.0006 across T-30 to T-90 — essentially flat. Sybil hunters build identifiable behavioral fingerprints (collection diversity, unique contract breadth) long before they claim rewards, and those fingerprints do not change as the event approaches.

**4. Evasion is economically self-defeating.**
Reducing behavioral diversity by 90% to evade detection drops AUC by only 0.6% — but costs the attacker 67% of accumulated incentive points. The detection system is game-theoretically stable: rational sybil hunters cannot evade without destroying their own reward.

---

ARTEMIS (WWW'24) asks: who claimed the airdrop sybilically? We ask a different question: can we identify them before they claim, using only what was observable on-chain in advance? The answer is yes, substantially, and months in advance — on both NFT and bridge protocols.

## Key Results

| Finding | Detail |
|---------|--------|
| **Pre-airdrop detection beats post-hoc GNN** | Blur: AUC 0.905 (T-30). LayerZero: AUC 0.946 (T-30). Both exceed post-hoc ARTEMIS 0.803 using only pre-airdrop data |
| **GNN fails structurally before airdrop** | ArtemisNet collapses from 0.976 (post-hoc) to 0.586 (T-30) — architectural failure, not tuning |
| **Signal flat for 6 months** | Blur: 0.013 drop from T-0 to T-180. LayerZero: 0.0006 range across T-30/60/90 |
| **Evasion is economically self-defeating** | 90% diversity reduction: AUC drops 0.6%, incentive points drop 67% |
| **In-domain: robust across two protocol types** | Blur 0.905, LayerZero 0.946 — both with protocol-specific training |
| **Zero-shot transfer: limited without target data** | Same-class bridge (Hop→LZ): 0.567. Cross-domain (Blur→LZ): 0.434. Common-feature set improves Blur→Hop to 0.78 |
| **Two-stage detector handles unseen sybil types** | BW-type left out: supervised AUC 0.047 → Isolation Forest AUC 0.916 |

> **Adversarial robustness highlight:** Sybil evasion is economically irrational. A sybil hunter who reduces behavioral diversity by 90% to evade detection loses 67% of their accumulated incentive points, while AUC drops by only 0.6%. The economic cost of evasion far exceeds any benefit, making the detection system game-theoretically stable.

## Why GNN Does Not Work Pre-Airdrop

ArtemisNet (GNN) achieves AUC 0.976 post-hoc but collapses to AUC 0.586 at T-30. The reason is architectural: ArtemisNet's graph edges include transfer events generated when the airdrop is distributed. Remove those edges and the graph is structurally incomplete. This is not a tuning issue; it is a fundamental incompatibility between graph-based post-hoc methods and the pre-airdrop setting. LightGBM on behavioral features does not have this dependency.

## Experimental Design

The 28 experiments are organized around five questions.

**Can we detect before the airdrop, and how early?** Experiments 01-03 and 12 establish the main result and temporal ablation from T-0 to T-180. Data leakage is prevented by a strict timestamp cutoff on all feature computation.

**Which features matter and why?** Experiments 04, 06, 10, and 19 characterize feature importance from three angles: LightGBM gain-based importance across time windows, group ablation (Activity / Diversity / Volume / Behavioral / DeFi), and SHAP value distributions. DeFi features contribute almost nothing (AUC 0.544 alone) because the sybil strategy centers on NFT transactions, not Blur's Blend lending product.

**How well does the model actually generalize?** Experiments 05, 08, 11, 20, 21, and 22 test generalization from multiple directions: precision-recall thresholds for deployment, temporal and population splits, sybil subtype clustering (three types: retail hunter, mid-volume, and six hyperactive bots), leave-one-flag-out generalization by sybil strategy, and probability calibration. The critical failure case is flag-type generalization: when BW (high-value buyer) sybils are excluded from training, AUC drops to 0.047, because the model never learned to associate high buy_value with sybil behavior.

**What is the scope of the approach?** Experiments 14-18, 24, and 28 establish two separate claims with different strength levels. First, in-domain detection is robust: when trained on target-protocol data, the approach achieves AUC 0.905 on Blur and 0.946 on LayerZero (Exp 28), confirming that sybil behavioral fingerprints exist and are detectable in both NFT and bridge protocols. Second, zero-shot transfer is limited: without any target-protocol training data, AUC ranges from 0.43 to 0.57 (Exps 16/24). This gap is primarily due to feature mismatch — with a five-feature common set, Blur-to-Hop improves from 0.38 to 0.78 (Exp 17). The conclusion is not that the method generalizes zero-shot, but that the underlying signal is real in every protocol tested and is recoverable with modest target-domain supervision (1% labels recover AUC to 0.982 on Hop, Exp 14).

**Are the weaknesses addressable?** Experiments 25-27 directly address the three main limitations. Isolation Forest (unsupervised) raises detection of the unseen BW type from 0.047 to 0.916, because BW sybils are statistical outliers in feature space regardless of labeling. A two-stage deployment (supervised LightGBM plus unsupervised IF) is robust to novel sybil strategies. Label noise experiments show AUC drops only 0.014 under 20% label corruption, meaning the model is deployable even when the official blacklist is imperfect. Rule-based baselines (wallet age, transaction count thresholds) peak at AUC 0.610, far below LightGBM's 0.905, confirming that simple heuristics cannot capture the composite behavioral patterns that distinguish sybils from genuine heavy users.

## Datasets and Data Sources

Each dataset uses a different source for on-chain features. This section documents exactly where the data comes from.

### Blur (primary dataset)

| Item | Detail |
|------|--------|
| Source | Blur NFT marketplace transaction logs, Season 2 (Feb-Nov 2023) |
| Raw data | `TXS2_1662_1861.csv` — 3.2M NFT transactions, 251K unique addresses |
| Labels | Blur official `airdrop2_targets` sybil list — binary is_sybil |
| T0 | 1700525735 (2023-11-21 00:15:35 UTC, first Season 2 claim) |
| Feature pipeline | `01_build_features.py` — 18 features computed directly from raw transaction CSV, no external API calls |

### Hop Protocol (cross-protocol, Exp 14/16/17)

| Item | Detail |
|------|--------|
| Source | Hop bridge transaction history |
| Labels | Hop Sybil Hunter program official list |
| Feature pipeline | Protocol-specific features; common 5-feature set for cross-protocol transfer (tx_count, total_volume, wallet_age_days, unique_contracts, active_span_days) |

### Gitcoin GR15 (cross-protocol, Exp 16/17/18)

| Item | Detail |
|------|--------|
| Source | Gitcoin Grants Round 15 on-chain data |
| Labels | SADScore — Sybil Address Detection scores as binary labels |
| Addresses | 39,962 addresses |
| On-chain features | Fetched via **Etherscan V2 API** (`api.etherscan.io/v2/api`), multiple keys in parallel (`fetch_gitcoin_onchain.mjs`) |

### LayerZero ZRO (cross-protocol, Exp 24 and Exp 28)

**Exp 24** (`multichain_features.csv`) and **Exp 28** (`lz_temporal_features.csv`) use the same address list but different feature pipelines:

| Item | Detail |
|------|--------|
| Addresses | 29,849 total; 19,480 active (total_tx > 0); 9,899 sybil / 9,581 normal |
| Labels | LayerZero official "Proof of Sybil" campaign results |
| T0 | 1718841600 (2024-06-20 00:00 UTC, ZRO airdrop) |

**Exp 24 features** (`multichain_features.csv`):
- Fetched via **Etherscan V2 API** across 3 chains: ETH mainnet, Arbitrum, Polygon
- Keys: two rotating Etherscan API keys, 5 req/s each
- Script: `fetch_lz_features.mjs` — aggregated chain-level features summed across chains
- Covers: tx_count, wallet_age_days, unique_contracts, active_span_days, total_volume per chain and cross-chain total

**Exp 28 features** (`lz_temporal_features.csv`):
- Fetched via **Alchemy API** (`alchemy_getAssetTransfers`, ETH mainnet only)
- Alchemy key used: `dPr_w2ES924h7eiMOHbeM` (app at dashboard.alchemy.com)
- Script: `fetch_lz_alchemy.mjs` — 30 concurrent workers, ~100 addresses/second
- Temporal cutoffs computed from same data: T-30 (1716249600), T-60 (1713657600), T-90 (1711065600)
- **Limitation**: ARB and POLY networks not yet enabled in the Alchemy app. Exp 28 features are ETH-mainnet only. Re-running after enabling ARB/POLY will produce more complete cross-chain features.

### Known Data Limitation (Exp 28)

Exp 28 uses ETH-only behavioral features for a bridge protocol (LayerZero) whose core utility is cross-chain. This means the T-30/60/90 features capture Ethereum-side activity (token approvals, contract interactions before bridging) but miss the destination-chain behavior. Despite this limitation, Exp 28 achieves AUC 0.946, suggesting the pre-bridge Ethereum-side behavior alone is sufficient for detection. Adding ARB/POLY features is expected to improve results further.

## Experiments

Experiment map with figures and results: [MINDMAP.html](https://tyche1107.github.io/pre-airdrop-detection/MINDMAP.html)

### Detection Horizon

How early can we detect, and does the signal degrade over time?

| Script | Purpose | Key result |
|--------|---------|------------|
| 01_build_features.py | Extract 18 behavioral features from raw transactions at each temporal cutoff, enforcing strict timestamp isolation | Preprocessing for all downstream experiments |
| 02_train_lightgbm.py | LightGBM 5-fold CV on T-30 data, compared against ARTEMIS | AUC 0.905 vs ARTEMIS 0.803 |
| 03_temporal_ablation.py | Sweep AUC from T-0 to T-90 in six windows | T-0: 0.908, T-30: 0.905, T-90: 0.902 |
| 12_extended_temporal.py | Extend the sweep to T-120, T-150, T-180 | T-180: 0.895, total drop over six months is 0.013 |

### Feature Analysis

Which of the 18 features carry signal, and does their importance shift across time windows?

| Script | Purpose | Key result |
|--------|---------|------------|
| 04_feature_importance.py | Track LightGBM gain-based importance at T-0, T-30, T-60, T-90 | NFT diversity stable at 38-49%; unique_interactions rises from 7% to 22% as window extends |
| 06_ablation_features.py | Remove one feature group at a time: Activity, Diversity, Volume, Behavioral, DeFi | DeFi alone achieves 0.544; sybils do not use Blend |
| 07_graph_features.py | Add graph structure features (address clustering, neighbor counts) to LightGBM | Behavioral 0.904, behavioral+graph 0.905; graph adds 0.001 |
| 10_shap_analysis.py | SHAP beeswarm on T-30 model | Confirms unique_interactions and buy_value as top contributors alongside diversity |
| 19_shap_temporal.py | Compare SHAP distributions at T-90 vs T-7 | T-90: wallet_age_days high (batch wallet creation signal); T-7: recent_activity high (sprint-before-distribution signal) |

### GNN Comparison

Why does the best existing method fail in the pre-airdrop setting?

| Script | Purpose | Key result |
|--------|---------|------------|
| 09_artemis_gnn.py | Reproduce ArtemisNet GNN with full post-hoc graph | AUC 0.976, confirms the post-hoc upper bound |
| 15_gnn_preairdrop.py | Apply ArtemisNet to pre-airdrop graph (T-30 cutoff) | AUC 0.586; collapses because airdrop transfer edges are absent |

### Generalization and Robustness

Does the model learn a real signal or overfit to Blur Season 2?

| Script | Purpose | Key result |
|--------|---------|------------|
| 05_pr_analysis.py | Full precision-recall curve for threshold selection at deployment | Documents trade-off across operating points |
| 08_generalization.py | Temporal split (train T-90, test T-30) and population split (50% unseen sybil types) | Temporal: 0.898; population: 0.744 |
| 11_sybil_clustering.py | K-means on sybil addresses to identify behavioral subtypes | K=3: 49K retail hunters, 601 mid-volume, 6 hyperactive bots (9K+ transactions each) |
| 20_clustering_k_selection.py | Elbow and silhouette analysis to justify K choice | Silhouette optimal at K=2 (0.984), K=3 chosen for semantic separation |
| 21_flag_type_generalization.py | Leave-one-flag-out: train without one sybil type, test on that type | BW=0.047, FD=0.110, ML=0.241, HF=0.550; each type has a distinct behavioral fingerprint |
| 22_calibration.py | Isotonic regression calibration of output probabilities | Brier score 0.120 to 0.091 (-24%), AUC unchanged |

### Adversarial Analysis

If a sybil hunter knows about the model, can they cheaply evade it?

| Script | Purpose | Key result |
|--------|---------|------------|
| 13_adversarial_robustness.py | Simulate diversity-reduction evasion at increasing levels | AUC drops slowly under evasion; 70% diversity reduction cuts AUC to 0.901 |
| 23_adversarial_cost.py | Quantify the economic cost of evasion in lost airdrop points | 90% diversity cut reduces AUC by 0.6% but costs 67% of expected points; evasion is self-defeating |

### Cross-Protocol Transfer

Does the method work on other protocols, and what determines transfer quality?

| Script | Purpose | Key result |
|--------|---------|------------|
| 14_cross_protocol.py | Blur-to-Hop zero-shot and fine-tune with protocol-specific features | Zero-shot AUC 0.500; 1% Hop labels recovers 0.982 |
| 16_lopo_crossprotocol.py | Leave-one-protocol-out across Blur, Hop, Gitcoin with protocol-specific features | AUC 0.22-0.47; initially looks like failure |
| 16b_lopo_blur_hop.py | Blur-Hop label-budget curve: how many Hop labels are needed? | 1% labels (roughly 300 samples) recovers AUC 0.982; 20% gives 0.981 |
| 17_common_features_lopo.py | Repeat LOPO with five protocol-agnostic features | Blur-to-Hop improves from 0.38 to 0.78; low LOPO AUC in Exp 16 was mostly feature mismatch |
| 18_gitcoin_feature_importance.py | Feature importance within the Gitcoin domain | gitcoin_donations importance = 0%; SADScore labels general on-chain anomalies, not Gitcoin-specific farming |
| 24_layerzero_lopo.py | Add LayerZero as fourth protocol; test same-class vs cross-domain transfer | Hop-to-LZ (bridge-to-bridge): 0.567; Blur-to-LZ (NFT-to-bridge): 0.434; protocol category affects transfer |
| 28_layerzero_temporal.py | Full temporal ablation on LayerZero (17,072 active addresses at T-30); same pipeline as Blur Exp 03 | T-30: AUC 0.9462, P 0.8648, R 0.8786, F1 0.8717; T-60: 0.9468 (16,781); T-90: 0.9462 (15,616) — range 0.0006, flatter than Blur; exceeds LZ in-domain bound from Exp 24 (0.892) |

### Deployment Readiness

Can the remaining weaknesses be resolved before production use?

| Script | Purpose | Key result |
|--------|---------|------------|
| 25_openworld_detection.py | Two-stage detector: LightGBM for known types, Isolation Forest for unknown | IF raises BW detection from 0.047 to 0.916 without any BW labels |
| 26_label_noise.py | Flip 5-20% of labels randomly, retrain, measure AUC degradation | 20% label corruption drops AUC by 0.014; model is robust to imperfect blacklists |
| 27_rule_baselines.py | Compare against simple heuristics (wallet age, tx count) and weaker ML models | Best rule: 0.610; Logistic Regression: 0.859; Random Forest: 0.897; LightGBM: 0.905 |

## Reproducibility

All temporal cutoffs are enforced by filtering on raw transaction timestamps before any feature computation. Features at T-k use only transactions with timestamp less than (T0 minus k days in seconds). Labels are applied after feature computation and are never used to select which transactions to include. The Blur T0 is 1700525735 (first Season 2 claim transaction, 2023-11-21 00:15:35 UTC).
